{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "001caa2b",
   "metadata": {},
   "source": [
    "O objetivo deste notebook é obter um bom modelo de predição de uso de oferta.\n",
    "\n",
    "Lembre-se de que:\n",
    "\n",
    "- Não se pode utilizar dados de um mesmo cliente no treino, validação e teste ao mesmo tempo\n",
    "- Pode-se utilizar modelos de arvore para, por exemplo, entregar a probabilidade de o cliente utilizar o cupom\n",
    "- Um modelo por cluster pode melhorar os resultados?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c884cf7",
   "metadata": {},
   "source": [
    "# 1. Importando bibliotecas e dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e9c22d63",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from joblib import dump\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, log_loss\n",
    "\n",
    "# Modelos de classificação\n",
    "from catboost import CatBoostClassifier\n",
    "\n",
    "import sys\n",
    "sys.path.append('../src/')\n",
    "\n",
    "from Model import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8cbc3f87",
   "metadata": {},
   "outputs": [],
   "source": [
    "offer_for_modeling = pd.read_csv(\"../data/processed/offers_for_modeling.csv\")\n",
    "# Target\n",
    "y_col = [\"TARGET_USED_OFFER\"]\n",
    "\n",
    "# Coluna dos clusters\n",
    "cluster_col = \"cluster\"\n",
    "\n",
    "# Coluna que separa nosso dataset dentro dos clusters\n",
    "partition_col = \"account_id\"\n",
    "\n",
    "# Colunas endógenas (temos controle)\n",
    "endogenous_cols = [\n",
    "    \"offer_received_date\", \"channel_mobile\", \"channel_email\", \"channel_social\",\n",
    "    \"channel_web\", \"discount_value\", \"offer_duration\", \"offer_min_value\"\n",
    "]\n",
    "endogenous_cat_cols = [\"offer_type\"]\n",
    "\n",
    "# Colunas exógenas (externas, não temos controle)\n",
    "exogenous_cols = [\n",
    "    \"recently_viewed_info_offer\",\"age\",\"credit_card_limit\",\"total_transactions\",\"avg_amount\",\n",
    "    \"most_used_offer_type\",\"transactions_offer_rate\",\"avg_time_to_use_offer\",\"avg_time_to_view_offer\",\n",
    "    \"all_coupon_usage_rate\",\"pct_used_channel_mobile\",\"pct_used_channel_email\",\"pct_used_channel_social\",\n",
    "    \"pct_used_channel_web\",\"pct_used_type_bogo\",\"pct_used_type_discount\",\"pct_used_type_informational\",\n",
    "    \"all_coupon_viewed_rate\",\"pct_viewed_channel_mobile\",\"pct_viewed_channel_email\",\"pct_viewed_channel_social\",\n",
    "    \"pct_viewed_channel_web\",\"pct_viewed_type_bogo\",\"pct_viewed_type_discount\",\"pct_viewed_type_informational\",\n",
    "    \"total_offers_received\",\"total_mobile_offer\",\"total_email_offer\",\"total_social_offer\",\n",
    "    \"total_web_offer\",\"total_offers_bogo\",\"total_offers_discount\",\"total_offers_informational\"\n",
    "]\n",
    "\n",
    "# Variáveis categóricas\n",
    "categorical_features = [\n",
    "    \"age\",\"credit_card_limit\",\"total_transactions\",\"avg_amount\",\n",
    "    \"most_used_offer_type\",\"transactions_offer_rate\",\"avg_time_to_use_offer\",\"avg_time_to_view_offer\",\n",
    "    \"all_coupon_usage_rate\",\"pct_used_channel_mobile\",\"pct_used_channel_email\",\"pct_used_channel_social\",\n",
    "    \"pct_used_channel_web\",\"pct_used_type_bogo\",\"pct_used_type_discount\",\"pct_used_type_informational\",\n",
    "    \"all_coupon_viewed_rate\",\"pct_viewed_channel_mobile\",\"pct_viewed_channel_email\",\"pct_viewed_channel_social\",\n",
    "    \"pct_viewed_channel_web\",\"pct_viewed_type_bogo\",\"pct_viewed_type_discount\",\"pct_viewed_type_informational\",\n",
    "    \"total_offers_received\",\"total_mobile_offer\",\"total_email_offer\",\"total_social_offer\",\n",
    "    \"total_web_offer\",\"total_offers_bogo\",\"total_offers_discount\",\"total_offers_informational\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "acd55e98",
   "metadata": {},
   "outputs": [],
   "source": [
    "account_cluster = pd.read_csv(\"../data/processed/profile_clustered_id.csv\")\n",
    "\n",
    "# Join para acrescentar cluster na tabela antes da modelagem\n",
    "offer_for_modeling = (pd.merge(offer_for_modeling, account_cluster,\n",
    "    on=\"account_id\", how=\"left\")\n",
    ")\n",
    "\n",
    "del(account_cluster)\n",
    "\n",
    "# Aplicar one-hot encoding apenas nas colunas endógenas\n",
    "df_encoded = pd.get_dummies(offer_for_modeling[endogenous_cat_cols], prefix=endogenous_cat_cols, drop_first=False, dtype=float)\n",
    "\n",
    "# Definindo todas as variáveis do modelo\n",
    "X_cols = endogenous_cols + list(df_encoded.columns) + exogenous_cols\n",
    "\n",
    "# Concatenar com as colunas exógenas\n",
    "offer_for_modeling = pd.concat(\n",
    "    [\n",
    "        offer_for_modeling,\n",
    "        df_encoded\n",
    "    ],\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# Coletando todos os clusters\n",
    "unique_clusters = list(offer_for_modeling[cluster_col].sort_values().unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93460b9b",
   "metadata": {},
   "source": [
    "# 2. Analise pré treino"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbc77aa5",
   "metadata": {},
   "source": [
    "## 2.1. Verificando quantidade de dados por cluster\n",
    "\n",
    "Verificando se mesmo ao separar os dados de acordo com os clusters, temos dados suficientes para treinar modelos de ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6d5c42d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "cluster\n",
       "0    320.604651\n",
       "1    144.930233\n",
       "2    186.418605\n",
       "3    171.139535\n",
       "4    798.860465\n",
       "5    193.558140\n",
       "6    127.372093\n",
       "7    228.767442\n",
       "Name: count, dtype: float64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "offer_for_modeling[\"cluster\"].value_counts().sort_index() / len(X_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eccfb3b",
   "metadata": {},
   "source": [
    "Vemos que para todos os 8 clusters temos mais de 10 linhas para cada coluna, passando do mínimo recomendado para treino de modelos de ML"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38d2b05e",
   "metadata": {},
   "source": [
    "## 2.2. Removendo clusters com target constantes e balanceando targets\n",
    "\n",
    "Considerando que alguns clusters não usam ofertas, vamos remover os que não usam\n",
    "\n",
    "E balancear a target nos clusters que usam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f1c100a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster 0 será removido da lista de treino porque não possui uso de ofertas.\n",
      "\n",
      "Distribuição da variável target para o cluster 1\n",
      "- Positivo: 92% (5,723 → 133 linhas por coluna)\n",
      "- Negativo: 8% (509 → 12 linhas por coluna)\n",
      "\n",
      "Distribuição da variável target para o cluster 2\n",
      "- Positivo: 34% (2,715 → 63 linhas por coluna)\n",
      "- Negativo: 66% (5,301 → 123 linhas por coluna)\n",
      "\n",
      "Cluster 3 será removido da lista de treino porque não possui uso de ofertas.\n",
      "\n",
      "Distribuição da variável target para o cluster 4\n",
      "- Positivo: 93% (31,950 → 743 linhas por coluna)\n",
      "- Negativo: 7% (2,401 → 56 linhas por coluna)\n",
      "\n",
      "Distribuição da variável target para o cluster 5\n",
      "- Positivo: 75% (6,252 → 145 linhas por coluna)\n",
      "- Negativo: 25% (2,071 → 48 linhas por coluna)\n",
      "\n",
      "Distribuição da variável target para o cluster 6\n",
      "- Positivo: 31% (1,685 → 39 linhas por coluna)\n",
      "- Negativo: 69% (3,792 → 88 linhas por coluna)\n",
      "\n",
      "Distribuição da variável target para o cluster 7\n",
      "- Positivo: 64% (6,328 → 147 linhas por coluna)\n",
      "- Negativo: 36% (3,509 → 82 linhas por coluna)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for _, cl in enumerate(unique_clusters.copy()):\n",
    "    if offer_for_modeling[offer_for_modeling[\"cluster\"] == cl][y_col[0]].sum() == 0:\n",
    "        print(f\"Cluster {cl} será removido da lista de treino porque não possui uso de ofertas.\")\n",
    "        print()\n",
    "        unique_clusters.remove(cl)\n",
    "    else:\n",
    "        # Printando informações para ajudar a entender o que esta acontecendo\n",
    "        print(f\"Distribuição da variável target para o cluster {cl}\")\n",
    "        target_count = offer_for_modeling[offer_for_modeling[\"cluster\"] == cl][y_col[0]].value_counts().to_dict()\n",
    "        target_dist = offer_for_modeling[offer_for_modeling[\"cluster\"] == cl][y_col[0]].value_counts(normalize=True).to_dict()\n",
    "        print(f\"- Positivo: {round(target_dist[1]*100)}% ({target_count[1]:,} → {round(target_count[1] / len(X_cols))} linhas por coluna)\")\n",
    "        print(f\"- Negativo: {round(target_dist[0]*100)}% ({target_count[0]:,} → {round(target_count[0] / len(X_cols))} linhas por coluna)\")\n",
    "        print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd94f8cc",
   "metadata": {},
   "source": [
    "Vemos que 2 clusters foram exlcuidos e alguns estão muito desbalanceados, portanto vamos precisar balancear para evitar overfit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5864b47",
   "metadata": {},
   "source": [
    "# 3. Separação de dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "784f7704",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster 1 (ñ balanceado): Treino 4,322 (69% → 3,976 | 346) | Validação 1,910 (31% → 1,747 | 163)\n",
      "Cluster 1   (balanceado): Treino 692 (68% → 346 | 346) | Validação 326 (32% → 163 | 163)\n",
      "\n",
      "Cluster 2 (ñ balanceado): Treino 5,627 (70% → 1,901 | 3,726) | Validação 2,389 (30% → 814 | 1,575)\n",
      "Cluster 2   (balanceado): Treino 3,802 (70% → 1,901 | 1,901) | Validação 1,628 (30% → 814 | 814)\n",
      "\n",
      "Cluster 4 (ñ balanceado): Treino 24,039 (70% → 22,319 | 1,720) | Validação 10,312 (30% → 9,631 | 681)\n",
      "Cluster 4   (balanceado): Treino 3,440 (72% → 1,720 | 1,720) | Validação 1,362 (28% → 681 | 681)\n",
      "\n",
      "Cluster 5 (ñ balanceado): Treino 5,858 (70% → 4,456 | 1,402) | Validação 2,465 (30% → 1,796 | 669)\n",
      "Cluster 5   (balanceado): Treino 2,804 (68% → 1,402 | 1,402) | Validação 1,338 (32% → 669 | 669)\n",
      "\n",
      "Cluster 6 (ñ balanceado): Treino 3,865 (71% → 1,177 | 2,688) | Validação 1,612 (29% → 508 | 1,104)\n",
      "Cluster 6   (balanceado): Treino 2,354 (70% → 1,177 | 1,177) | Validação 1,016 (30% → 508 | 508)\n",
      "\n",
      "Cluster 7 (ñ balanceado): Treino 6,905 (70% → 4,444 | 2,461) | Validação 2,932 (30% → 1,884 | 1,048)\n",
      "Cluster 7   (balanceado): Treino 4,922 (70% → 2,461 | 2,461) | Validação 2,096 (30% → 1,048 | 1,048)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "modeling_data = {}\n",
    "\n",
    "for cl in unique_clusters:\n",
    "    # Filtrando cluster desejado\n",
    "    cl_offers = offer_for_modeling[offer_for_modeling[\"cluster\"] == cl]\n",
    "\n",
    "    cl_account_ids = cl_offers[partition_col].unique()\n",
    "\n",
    "    # Separar dados de treino/teste por account_id\n",
    "    train_ids, val_ids = train_test_split(cl_account_ids, test_size=0.3, random_state=3)\n",
    "\n",
    "    # Dados de treino\n",
    "    df_train = cl_offers[cl_offers[\"account_id\"].isin(train_ids)].reset_index(drop=True)\n",
    "    # Balanceando dados de treino\n",
    "    target_count = df_train[df_train[\"cluster\"] == cl][y_col[0]].value_counts().to_dict()\n",
    "    small_cat = target_count[1] if target_count[1] < target_count[0] else target_count[0]\n",
    "    df_train_bal = pd.concat(\n",
    "        [df_train[df_train[y_col[0]] == 1].sample(small_cat, random_state=3),\n",
    "         df_train[df_train[y_col[0]] == 0].sample(small_cat, random_state=3)]\n",
    "    )\n",
    "    # Dados de validação\n",
    "    df_val = cl_offers[cl_offers[\"account_id\"].isin(val_ids)].reset_index(drop=True)\n",
    "    # Balanceando dados de validação\n",
    "    target_count = df_val[df_val[\"cluster\"] == cl][y_col[0]].value_counts().to_dict()\n",
    "    small_cat = target_count[1] if target_count[1] < target_count[0] else target_count[0]\n",
    "    df_val_bal = pd.concat(\n",
    "        [df_val[df_val[y_col[0]] == 1].sample(small_cat, random_state=3),\n",
    "         df_val[df_val[y_col[0]] == 0].sample(small_cat, random_state=3)]\n",
    "    )\n",
    "\n",
    "    print_info = {\n",
    "        # Não balanceado\n",
    "        \"train_size\":   f\"{round(len(df_train)):,}\",\n",
    "        \"val_size\":     f\"{round(len(df_val)):,}\",\n",
    "        \"train_p\":      f\"{round(len(df_train)*100 / (len(df_train) + len(df_val))):,}%\",\n",
    "        \"val_p\":        f\"{round(len(df_val)*100 / (len(df_train) + len(df_val))):,}%\",\n",
    "        \"train_size_0\": f\"{round(len(df_train[df_train[y_col[0]] == 0])):,}\",\n",
    "        \"train_size_1\": f\"{round(len(df_train[df_train[y_col[0]] == 1])):,}\",\n",
    "        \"val_size_0\":   f\"{round(len(df_val[df_val[y_col[0]] == 0])):,}\",\n",
    "        \"val_size_1\":   f\"{round(len(df_val[df_val[y_col[0]] == 1])):,}\",\n",
    "        # Balanceado\n",
    "        \"train_bal_size\":   f\"{round(len(df_train_bal)):,}\",\n",
    "        \"val_bal_size\":     f\"{round(len(df_val_bal)):,}\",\n",
    "        \"train_bal_p\":      f\"{round(len(df_train_bal)*100 / (len(df_train_bal) + len(df_val_bal))):,}%\",\n",
    "        \"val_bal_p\":        f\"{round(len(df_val_bal)*100 / (len(df_train_bal) + len(df_val_bal))):,}%\",\n",
    "        \"train_bal_size_0\": f\"{round(len(df_train_bal[df_train_bal[y_col[0]] == 0])):,}\",\n",
    "        \"train_bal_size_1\": f\"{round(len(df_train_bal[df_train_bal[y_col[0]] == 1])):,}\",\n",
    "        \"val_bal_size_0\":   f\"{round(len(df_val_bal[df_val_bal[y_col[0]] == 0])):,}\",\n",
    "        \"val_bal_size_1\":   f\"{round(len(df_val_bal[df_val_bal[y_col[0]] == 1])):,}\",\n",
    "    }\n",
    "    print(f\"Cluster {cl} (ñ balanceado): Treino {print_info['train_size']} ({print_info['train_p']} → {print_info['train_size_1']} | {print_info['train_size_0']}) | Validação {print_info['val_size']} ({print_info['val_p']} → {print_info['val_size_1']} | {print_info['val_size_0']})\")\n",
    "    print(f\"Cluster {cl}   (balanceado): Treino {print_info['train_bal_size']} ({print_info['train_bal_p']} → {print_info['train_bal_size_1']} | {print_info['train_bal_size_0']}) | Validação {print_info['val_bal_size']} ({print_info['val_bal_p']} → {print_info['val_bal_size_1']} | {print_info['val_bal_size_0']})\")\n",
    "    print()\n",
    "\n",
    "    # Definindo X e y\n",
    "    modeling_data[cl] = {\n",
    "        \"X_train\": df_train[X_cols], # Com target não balanceada\n",
    "        \"y_train\": df_train[y_col],\n",
    "        \"X_train_bal\": df_train_bal[X_cols], # Com target balanceada\n",
    "        \"y_train_bal\": df_train_bal[y_col],\n",
    "\n",
    "        \"X_val\": df_val[X_cols], # Com target não balanceada\n",
    "        \"y_val\": df_val[y_col],\n",
    "        \"X_val_bal\": df_val_bal[X_cols], # Com target balanceada\n",
    "        \"y_val_bal\": df_val_bal[y_col],\n",
    "\n",
    "        \"metrics\": {},\n",
    "        \"metrics_full_table\": {},\n",
    "        \"params\": {},\n",
    "        \"model\": None\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de5777b5",
   "metadata": {},
   "source": [
    "Com essa separação dos dados por account_id temos o beneficio de não correr o risco de ter um bom resultado por causa da similaridade de comportamento de um usuário que esta na base de treino e validação ao mesmo tempo.\n",
    "\n",
    "Seria um problema utilizar essa abordagem de separação se os dados de treino e teste ficassem com tamanhos muito diferentes de 70/30, mas como vimos no print anterior, todos os clusters obtiveram uma boa separação aleatória."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07567196",
   "metadata": {},
   "source": [
    "# 4. Otimizações de hiperparametros"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e36a25e",
   "metadata": {},
   "source": [
    "A métrica a ser utilizada é a precisão, isso porque queremos monitorar se o modelo esta prevendo corretamente quando ele diz que a oferta será utilizada.\n",
    "\n",
    "Isso é importante pois queremos que ele detecte corretamente quais os tipos de oferta que devemos oferecer para cada cliente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b07c5fe9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████| 6/6 [31:29<00:00, 314.89s/it]\n"
     ]
    }
   ],
   "source": [
    "for cl in tqdm(unique_clusters[:], mininterval=10, ncols=100):\n",
    "    # Gerando instancia inicial do modelo\n",
    "    CBC = CatBoostClassifier(\n",
    "        verbose=False,\n",
    "        random_seed=3,\n",
    "        cat_features=categorical_features,\n",
    "        train_dir='tmp_catboost_info'\n",
    "    )\n",
    "\n",
    "    # Definindo parametros a serem testados\n",
    "    param_dist = {\n",
    "        \"depth\": [4, 6, 8],\n",
    "        \"learning_rate\": [0.01, 0.05, 0.1, 0.2],\n",
    "        \"iterations\": [100, 200, 500],\n",
    "        \"l2_leaf_reg\": [1, 3, 5, 7, 9],\n",
    "        \"border_count\": [32, 64, 128]\n",
    "    }\n",
    "\n",
    "    # Realizando testes aleatórios para detectar o mlehor resultado\n",
    "    random_search = RandomizedSearchCV(\n",
    "        CBC,\n",
    "        param_distributions=param_dist,\n",
    "        n_iter=15, # quantas combinações testar\n",
    "        scoring=\"precision\", # \"accuracy\", \"precision\"\n",
    "        cv=6, # Quantas vezes rodar validação cruzada\n",
    "        verbose=False,\n",
    "        n_jobs=-1,\n",
    "        random_state=3,\n",
    "    )\n",
    "\n",
    "    # Treinamento\n",
    "    random_search.fit(\n",
    "        modeling_data[cl][\"X_train_bal\"],\n",
    "        modeling_data[cl][\"y_train_bal\"]\n",
    "    )\n",
    "\n",
    "    # Melhor modelo\n",
    "    modeling_data[cl][\"params\"] = random_search.best_params_\n",
    "    modeling_data[cl][\"model\"] = random_search.best_estimator_\n",
    "\n",
    "    # Gerando demais métricas para o melhor modelo\n",
    "    pred_val = modeling_data[cl][\"model\"].predict(modeling_data[cl][\"X_val_bal\"])\n",
    "    modeling_data[cl][\"metrics\"] = {\n",
    "        \"precision\": precision_score(modeling_data[cl][\"y_val_bal\"], pred_val),\n",
    "        \"accuracy\": accuracy_score(modeling_data[cl][\"y_val_bal\"], pred_val),\n",
    "        \"recall\": recall_score(modeling_data[cl][\"y_val_bal\"], pred_val),\n",
    "        \"f1\": f1_score(modeling_data[cl][\"y_val_bal\"], pred_val),\n",
    "        \"log_loss\": log_loss(modeling_data[cl][\"y_val_bal\"], pred_val),\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        MODEL = Model(modeling_data[cl])\n",
    "        dump(MODEL, f\"../src/models/catboost_cl{cl}.joblib\")\n",
    "    except:\n",
    "        os.mkdir(\"../src/models\")\n",
    "        MODEL = Model(modeling_data[cl])\n",
    "        dump(MODEL, f\"../src/models/catboost_cl{cl}.joblib\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03dd1f56",
   "metadata": {},
   "source": [
    "# 5. Analisando as métricas dos modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a9048b24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analisando modelo do cluster 1\n",
      "- Precisão: 99%\n",
      "- Acurácia: 94%\n",
      "- Recall: 89%\n",
      "- F1: 94%\n",
      "\n",
      "\n",
      "Analisando modelo do cluster 2\n",
      "- Precisão: 90%\n",
      "- Acurácia: 92%\n",
      "- Recall: 95%\n",
      "- F1: 92%\n",
      "\n",
      "\n",
      "Analisando modelo do cluster 4\n",
      "- Precisão: 92%\n",
      "- Acurácia: 92%\n",
      "- Recall: 92%\n",
      "- F1: 92%\n",
      "\n",
      "\n",
      "Analisando modelo do cluster 5\n",
      "- Precisão: 93%\n",
      "- Acurácia: 91%\n",
      "- Recall: 88%\n",
      "- F1: 91%\n",
      "\n",
      "\n",
      "Analisando modelo do cluster 6\n",
      "- Precisão: 93%\n",
      "- Acurácia: 95%\n",
      "- Recall: 96%\n",
      "- F1: 95%\n",
      "\n",
      "\n",
      "Analisando modelo do cluster 7\n",
      "- Precisão: 94%\n",
      "- Acurácia: 94%\n",
      "- Recall: 94%\n",
      "- F1: 94%\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for cl in unique_clusters:\n",
    "    print(f\"Analisando modelo do cluster {cl}\")\n",
    "    print(f'- Precisão: {round(modeling_data[cl][\"metrics\"][\"precision\"]*100)}%')\n",
    "    print(f'- Acurácia: {round(modeling_data[cl][\"metrics\"][\"accuracy\"]*100)}%')\n",
    "    print(f'- Recall: {round(modeling_data[cl][\"metrics\"][\"recall\"]*100)}%')\n",
    "    print(f'- F1: {round(modeling_data[cl][\"metrics\"][\"f1\"]*100)}%')\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdb140d1",
   "metadata": {},
   "source": [
    "# 7. Desligando PC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "104c47b0",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'sleep'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msleep\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m sleep\n\u001b[32m      2\u001b[39m sleep(\u001b[32m20\u001b[39m)\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mos\u001b[39;00m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'sleep'"
     ]
    }
   ],
   "source": [
    "# from sleep import sleep\n",
    "# sleep(20)\n",
    "\n",
    "# import os\n",
    "# os.system(\"shutdown /s /t 0\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ifoodcase_311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
